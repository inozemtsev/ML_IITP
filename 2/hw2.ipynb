{
 "metadata": {
  "name": "",
  "signature": "sha256:5798533272e34e471db4a900fbaf699eb7b446e10b1ce5d1f6304a8f65e04b3c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1. Preparation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle, gzip, numpy\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.linear_model import LogisticRegression as LogReg\n",
      "from sklearn.linear_model import Perceptron as Percp\n",
      "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
      "from sklearn.tree import DecisionTreeClassifier as DecTree\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def linear_plot (w):\n",
      "\tpca_plot = PCA (n_components = 2)\n",
      "\tpca_res_plot = pca.fit_transform (pca_res)\n",
      "\tprint pca_res_plot.shape\n",
      "# \tplt.scatter (class1[:][0], class1[:][1], c='b')\n",
      "# \tplt.scatter (class2[:][0], class2[:][1], c='r')\n",
      "# \tplt.plot ([0,3], [-w[2]/w[1], (-w[2]-3*w[0])/w[1]])\n",
      "# \tplt.show()\n",
      "\n",
      "def get_data (data, class1, class2):\n",
      "\ttwo_classes = numpy.array([data[0][i] for i in numpy.where (data[1]==class1)[0]])\n",
      "\ty = numpy.zeros((two_classes.shape[0],1))\n",
      "\tsecond_class = numpy.array([data[0][i] for i in numpy.where (data[1]==class2)[0]])\n",
      "\ttwo_classes = numpy.vstack((two_classes, second_class))\n",
      "\ty = numpy.vstack ((y, numpy.ones((second_class.shape[0],1))))\n",
      "\ty=y.ravel()\n",
      "\treturn (two_classes,y)\t\n",
      "\n",
      "def apply_logreg (pen, Const=1.0):\n",
      "\tlogreg = LogReg (penalty = pen, C=Const)\n",
      "\tlogreg.fit (pca_res,y_train)\n",
      "\t#print logreg.coef_\n",
      "    linear_plot ([logreg.coef_, logreg.intercept_[0]])\n",
      "\treturn logreg.predict (numpy.dot(two_classes_test, pca.components_.T))\n",
      "\n",
      "def apply_percp (pen=None, a=0.0001, n=5):\n",
      "\tpercp = Percp (penalty = pen, alpha=a, n_iter=n)\n",
      "\tpercp.fit (pca_res, y_train)\n",
      "\treturn percp.predict (numpy.dot(two_classes_test, pca.components_.T))\n",
      "\n",
      "def apply_knn (neighbors=5, w='uniform'):\n",
      "\tknn = KNN (n_neighbors=neighbors, weights=w)\n",
      "\tknn.fit (pca_res, y_train)\n",
      "\treturn knn.predict (numpy.dot(two_classes_test, pca.components_.T))\n",
      "\n",
      "def apply_dectree (criterion='gini', splitter='best', max_features='auto'):\n",
      "\tdectree = DecTree (criterion = criterion, splitter = splitter)\n",
      "\tdectree.fit (pca_res, y_train)\n",
      "\treturn dectree.predict (numpy.dot(two_classes_test, pca.components_.T))\n",
      "\n",
      "def print_results ():\n",
      "\tprint 'Number of test samples:', ans.shape[0]\n",
      "\tprint 'Number of errors: ', numpy.sum(numpy.abs(ans-y_test))\n",
      "\tprint 'Percent of errors: ', numpy.sum(numpy.abs(ans-y_test))/ans.shape[0]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndentationError",
       "evalue": "unindent does not match any outer indentation level (<ipython-input-4-866de7c32106>, line 31)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-866de7c32106>\"\u001b[1;36m, line \u001b[1;32m31\u001b[0m\n\u001b[1;33m    linear_plot ([logreg.coef_, logreg.intercept_[0]])\u001b[0m\n\u001b[1;37m                                                      ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2. Loading dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = gzip.open('mnist.pkl.gz', 'rb')\n",
      "train_set, valid_set, test_set = cPickle.load(f)\n",
      "f.close()\n",
      "\n",
      "num1,num2=0,1\n",
      "two_classes_train, y_train = get_data (train_set, num1, num2)\n",
      "two_classes_test, y_test = get_data (test_set, num1, num2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "3. PCA."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print '----------------------------PCA------------------------'\n",
      "pca = PCA (n_components = 50) #because sum(explained_variance_ratio_) >= 0.9\n",
      "pca_res = pca.fit_transform (two_classes_train)\n",
      "print 'sum (explained_variance_ratio_): ', sum (pca.explained_variance_ratio_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "----------------------------PCA------------------------\n",
        "sum (explained_variance_ratio_): "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.903263362823\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4. Logistic Regression."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ans = apply_logreg ('l2', Const=0.01)\n",
      "print '---------------LogReg: penalty=l2--------------------'\n",
      "print_results()\n",
      "\n",
      "ans = apply_logreg ('l1', Const=0.1)\n",
      "print '---------------LogReg: penalty=l1--------------------'\n",
      "print_results()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "---------------LogReg: penalty=l2--------------------\n",
        "Number of test samples: 2115\n",
        "Number of errors:  7.0\n",
        "Percent of errors:  0.00330969267139\n",
        "---------------LogReg: penalty=l1--------------------"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of test samples: 2115\n",
        "Number of errors:  7.0\n",
        "Percent of errors:  0.00330969267139\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "5. Perceptron."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ans=apply_percp ()\n",
      "print '---------------Perceptron: penalty=None--------------------'\n",
      "print_results()\n",
      "\n",
      "ans=apply_percp ('l2', a=0.0000001)\n",
      "print '---------------Perceptron: penalty=l2--------------------'\n",
      "print_results()\n",
      "\n",
      "ans=apply_percp ('l1', a=0.000001)\n",
      "print '---------------Perceptron: penalty=l1--------------------'\n",
      "print_results()\n",
      "\n",
      "ans=apply_percp ('elasticnet', a=0.000001)\n",
      "print '---------------Perceptron: penalty=elasticnet--------------------'\n",
      "print_results()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "---------------Perceptron: penalty=None--------------------\n",
        "Number of test samples: 2115\n",
        "Number of errors:  6.0\n",
        "Percent of errors:  0.00283687943262\n",
        "---------------Perceptron: penalty=l2--------------------\n",
        "Number of test samples: 2115\n",
        "Number of errors:  6.0\n",
        "Percent of errors:  0.00283687943262\n",
        "---------------Perceptron: penalty=l1--------------------"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of test samples: 2115\n",
        "Number of errors:  6.0\n",
        "Percent of errors:  0.00283687943262\n",
        "---------------Perceptron: penalty=elasticnet--------------------\n",
        "Number of test samples: 2115\n",
        "Number of errors:  6.0\n",
        "Percent of errors:  0.00283687943262\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "6. KNN."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ans=apply_knn ()\n",
      "print '---------------KNN--------------------'\n",
      "print_results()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "---------------KNN--------------------\n",
        "Number of test samples: 2115\n",
        "Number of errors:  0.0\n",
        "Percent of errors:  0.0\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "7. Decision Tree."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ans=apply_dectree(max_features='log2')\n",
      "print '---------------DecisionTree--------------------'\n",
      "print_results()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "---------------DecisionTree--------------------\n",
        "Number of test samples: 2115\n",
        "Number of errors:  195.0\n",
        "Percent of errors:  0.0921985815603\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}